{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba41efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: load the checkpoints (autoencoder, diffusion, and classifier models)\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from lsdm.models.diffusion.ddim import DDIMSampler\n",
    "from omegaconf import OmegaConf\n",
    "from lsdm.util import instantiate_from_config\n",
    "import albumentations\n",
    "from torchvision import transforms\n",
    "from lsdm.data.general import DatasetWithClassifierAPIValidation\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "\"\"\"\n",
    "# TODO: create meta config file.\n",
    "device = \"cuda:0\"\n",
    "key_feature = \"others\"  # for new csv files, choose \"others\"\n",
    "lsdm_config = \"configs/latent-diffusion/lsdm_general.yaml\"\n",
    "classifier_config = \"configs/classifier/classifier_general.yaml\"\n",
    "classifier_ckpt = \"\"  # BreastMam classifier ckpt\n",
    "\n",
    "def load_model_from_config(config, ckpt, device=device):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=torch.device(\"cpu\"))#, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model.to(torch.device(device))  # load on cuda:1\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_lsdm_model():\n",
    "    config = OmegaConf.load(lsdm_config)  \n",
    "    model = load_model_from_config(config, config.model.params.unet_config.params.ckpt_path)\n",
    "    return model\n",
    "\n",
    "# load LSDM model (autoencoder+diffusion):\n",
    "lsdm_model = get_lsdm_model()\n",
    "# load the sampler with classifier model:\n",
    "sampler = DDIMSampler(lsdm_model,\n",
    "                      classifier_guidance=key_feature,\n",
    "                      classifier_config=classifier_config,\n",
    "                      classifier_ckpt=classifier_ckpt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0.1: randomly sample a batch from the dataset (only use the semantic maps for generation):\n",
    "\"\"\"\n",
    "Parameters:\n",
    "\"\"\"\n",
    "bs = 4\n",
    "dataset_dir = \"\"\n",
    "\n",
    "dataset = DatasetWithClassifierAPIValidation(\n",
    "    dataset_dir=dataset_dir,\n",
    "    data_csv_name=\"data/BreastMam_Test_coordinate_switched.csv\",\n",
    "    feature_name=\"pathology\",\n",
    "    slice_name=\"new_name\",\n",
    "    make_binary=False,\n",
    "    positive_threshold=1,\n",
    "    label_scale=1,\n",
    "    training_mode=True,\n",
    "    crop_size=64,\n",
    "    num_semantic_labels=3,  # [0,1,2]\n",
    "    abnormal_area_threshold=2,  # used in masks\n",
    "    mask_maxpooling_pixels=4,\n",
    "    random_rotation=False,\n",
    "    masked_image=False,\n",
    "    masked_guidance=True,\n",
    "    return_original_label=True,\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "# randomly sample a batch using PyTorch DataLoader and transfer the conditionings into a dict:\n",
    "for i, batch in enumerate(dataloader):\n",
    "    c_dict = {}\n",
    "    c_dict[\"c_spade\"] = [batch[\"label\"].to(\"cuda\")]\n",
    "    # for SPADE+concat models, add \"c_concat\" conditionings:\n",
    "    c_dict[\"c_concat\"] = [lsdm_model.get_learned_conditioning(batch[\"concat\"].to(\"cuda\"))]\n",
    "    c_dict[\"mask\"] = batch[\"mask\"]\n",
    "    c_dict[\"position\"] = batch[\"position\"]\n",
    "    c_dict[\"crop_size_half\"] = batch[\"crop_size_half\"]\n",
    "    break\n",
    "# show all the keys of the conditionings:\n",
    "print(c_dict.keys())\n",
    "# show all the file names picked as the current batch:\n",
    "print(batch[\"filename\"])\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(bs):\n",
    "    ax = fig.add_subplot(bs // 2,2,i+1)\n",
    "    ax.imshow(batch[\"original_image\"][i,:,:], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0.2: visualize the semantic maps:\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "for i in range(bs):\n",
    "    ax = fig.add_subplot(bs // 2,2,i+1)\n",
    "    ax.imshow(batch[\"original_label\"][i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14293b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: inference with classifier guidance:\n",
    "\"\"\"\n",
    "Parameters:\n",
    "\"\"\"\n",
    "sampling_steps = 20\n",
    "class_label = 0  # {0,1}\n",
    "guidance_scale = 20.\n",
    "\"\"\"\n",
    "parameters:\n",
    "\n",
    "-- S: number of sampling steps using DDIM.\n",
    "-- conditioning: all the useful conditional information as a dict.\n",
    "-- class_label: target classfier label. (0: benign; 1: malignant)\n",
    "-- batch_size: batch size of the generated samples.\n",
    "-- shape: shape of the generated latent.\n",
    "-- verbose: default: False.\n",
    "-- classifier_guidance_scale: key hyper-parameter of the classifier guidance. The scale of the guidance intensity\n",
    "\"\"\"\n",
    "samples_ddim, intermediates = sampler.sample_classifier_guidance(S=sampling_steps,\n",
    "                                                                 conditioning=c_dict,\n",
    "                                                                 class_label=class_label,  # 0: benign\n",
    "                                                                 batch_size=bs,\n",
    "                                                                 shape=[3, 128, 128],\n",
    "                                                                 verbose=False,\n",
    "                                                                 # alter this parameter to change the guidance scale:\n",
    "                                                                 classifier_guidance_scale=guidance_scale,\n",
    "                                                                 )\n",
    "# normalize:\n",
    "latent = (samples_ddim - torch.min(samples_ddim)) / (torch.max(samples_ddim) - torch.min(samples_ddim))\n",
    "latent = torch.permute(latent, (0,2,3,1)).cpu().numpy()\n",
    "latent = (latent * 255).astype(np.uint8)\n",
    "fig1 = plt.figure(figsize=(10, 10))\n",
    "for i in range(bs):\n",
    "    ax1 = fig1.add_subplot(bs // 2,2,i+1)\n",
    "    ax1.imshow(latent[i,:,:,:])\n",
    "    \n",
    "# generated images:\n",
    "lsdm_model.eval()\n",
    "x_samples_ddim = lsdm_model.decode_first_stage(samples_ddim)\n",
    "generated = (x_samples_ddim - torch.min(x_samples_ddim)) / (torch.max(x_samples_ddim) - torch.min(x_samples_ddim))\n",
    "generated = torch.permute(generated, (0,2,3,1)).cpu().numpy()\n",
    "fig2 = plt.figure(figsize=(10, 10))\n",
    "for i in range(bs):\n",
    "    ax2 = fig2.add_subplot(bs//2,2,i+1)\n",
    "    ax2.imshow(generated[i,:,:,:], cmap=\"gray\")\n",
    "    ax2.axis(\"off\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e3ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: inference with classifier guidance:\n",
    "\"\"\"\n",
    "Parameters:\n",
    "\"\"\"\n",
    "sampling_steps = 20\n",
    "class_label = 1  # {0,1}\n",
    "guidance_scale = 20.\n",
    "\"\"\"\n",
    "parameters:\n",
    "\n",
    "-- S: number of sampling steps using DDIM.\n",
    "-- conditioning: all the useful conditional information as a dict.\n",
    "-- class_label: target classfier label. (0: benign; 1: malignant)\n",
    "-- batch_size: batch size of the generated samples.\n",
    "-- shape: shape of the generated latent.\n",
    "-- verbose: default: False.\n",
    "-- classifier_guidance_scale: key hyper-parameter of the classifier guidance. The scale of the guidance intensity\n",
    "\"\"\"\n",
    "samples_ddim, intermediates = sampler.sample_classifier_guidance(S=sampling_steps,\n",
    "                                                                 conditioning=c_dict,\n",
    "                                                                 class_label=class_label,  # 0: benign\n",
    "                                                                 batch_size=bs,\n",
    "                                                                 shape=[3, 128, 128],\n",
    "                                                                 verbose=False,\n",
    "                                                                 # alter this parameter to change the guidance scale:\n",
    "                                                                 classifier_guidance_scale=guidance_scale,\n",
    "                                                                 )\n",
    "# normalize:\n",
    "latent = (samples_ddim - torch.min(samples_ddim)) / (torch.max(samples_ddim) - torch.min(samples_ddim))\n",
    "latent = torch.permute(latent, (0,2,3,1)).cpu().numpy()\n",
    "latent = (latent * 255).astype(np.uint8)\n",
    "fig1 = plt.figure(figsize=(10, 10))\n",
    "for i in range(bs):\n",
    "    ax1 = fig1.add_subplot(bs // 2,2,i+1)\n",
    "    ax1.imshow(latent[i,:,:,:])\n",
    "    \n",
    "# generated images:\n",
    "lsdm_model.eval()\n",
    "x_samples_ddim = lsdm_model.decode_first_stage(samples_ddim)\n",
    "generated = (x_samples_ddim - torch.min(x_samples_ddim)) / (torch.max(x_samples_ddim) - torch.min(x_samples_ddim))\n",
    "generated = torch.permute(generated, (0,2,3,1)).cpu().numpy()\n",
    "fig2 = plt.figure(figsize=(10, 10))\n",
    "for i in range(bs):\n",
    "    ax2 = fig2.add_subplot(bs//2,2,i+1)\n",
    "    ax2.imshow(generated[i,:,:,:], cmap=\"gray\")\n",
    "    ax2.axis(\"off\")\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
